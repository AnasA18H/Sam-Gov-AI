# ğŸ“‹ SAM.gov AI

> **AI-Powered Government Contract Analysis Platform**  
> Automating US Government contract solicitation analysis from SAM.gov

---

## ğŸ“‘ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [API Documentation](#-api-documentation)
- [Development](#-development)
- [Deployment](#-deployment)
- [How It Works](#-how-it-works)

---

## ğŸ¯ Overview

**SAM.gov AI** is an intelligent web application that automates the analysis of US Government contract solicitations from [SAM.gov](https://sam.gov). The platform streamlines the bid preparation process by automatically extracting critical information from solicitation documents, classifying opportunities, and providing actionable insights.

### Core Capabilities

| ğŸ”§ Capability | ğŸ“ Description |
|---------------|----------------|
| ğŸ” **Automated Scraping** | Extracts data directly from SAM.gov opportunity pages using Playwright |
| ğŸ“„ **Hybrid Document Analysis** | Table parsing for structured forms (SF1449, SF30) + LLM extraction for unstructured text (SOW, amendments) |
| ğŸ¤– **AI Classification** | Claude (Haiku) + Groq (Llama 3.1) powered classification of solicitations as Product/Service/Both with confidence scores |
| ğŸ“Š **CLIN Extraction** | Intelligent extraction of Contract Line Item Numbers with full product/service details, delivery requirements, and deadlines |
| ğŸ”¤ **Smart Text Extraction** | Google Document AI for scanned PDFs, pytesseract OCR fallback, pdfplumber for text-based PDFs |
| âš™ï¸ **Configurable Analysis** | Enable/disable document analysis and CLIN extraction via UI toggles |
| â° **Deadline Tracking** | LLM-based deadline extraction with timezone support and deduplication |
| ğŸ“¦ **Delivery Requirements** | Integrated extraction of delivery addresses, special delivery instructions, and delivery timelines |
| ğŸ”„ **Two-Pass Extraction** | Automatic second pass to fill missing fields when 20%+ of important fields are null |
| ğŸ›¡ï¸ **Robust JSON Parsing** | Advanced error recovery for malformed/truncated LLM responses |
| ğŸ‘¥ **Contact Management** | Automatic extraction and display of primary/alternative contacts |
| ğŸ­ **Manufacturer Research** | Two-phase research: Document extraction + LLM-guided external web search |
| ğŸª **Dealer Discovery** | Automated identification of top 8 authorized dealers with pricing, stock status, and legitimacy ranking |

---

## âœ¨ Features

### Phase 1 - Complete âœ…

<details>
<summary><strong>ğŸ” User Authentication & Security</strong></summary>

- âœ… Secure JWT-based authentication
- âœ… Password hashing with bcrypt
- âœ… Session management
- âœ… Protected routes and API endpoints

</details>

<details>
<summary><strong>ğŸŒ SAM.gov Integration</strong></summary>

- âœ… URL validation and opportunity ID extraction
- âœ… Playwright-based web scraping
- âœ… Automated attachment downloads (PDF, Word, Excel, ZIP)
- âœ… Contact information extraction (names, emails, phone numbers)
- âœ… Contracting office address capture

</details>

<details>
<summary><strong>ğŸ“š Advanced Document Analysis</strong></summary>

**Unified LLM Extraction:**
- âœ… All documents + SAM.gov page text combined into single LLM request
- âœ… Claude 3 Haiku (primary) + Groq Llama 3.1 (fallback)
- âœ… Two-pass extraction: Second pass automatically fills missing fields when 20%+ are null
- âœ… Robust JSON parsing with error recovery for malformed/truncated responses

**Smart Text Extraction:**
- âœ… Google Document AI for high-quality OCR on scanned PDFs
- âœ… pytesseract OCR with image preprocessing as fallback
- âœ… pdfplumber for text-based PDFs
- âœ… Support for PDF, Word, Excel, PowerPoint, Images, RTF, Markdown
- âœ… Raw text sent to LLM (no aggressive cleaning) to preserve all information

**Comprehensive CLIN Extraction:**
- âœ… Product/service details (name, description, manufacturer, part/model numbers, drawing numbers)
- âœ… Quantities and units of measure
- âœ… Scope of work (complete text extraction)
- âœ… Service requirements (detailed specifications)
- âœ… Delivery address (complete with facility name, street, city, state, ZIP)
- âœ… Special delivery instructions (testing requirements, schedules, constraints)
- âœ… Delivery timeline (complete phrases with dates and conditions)

**Additional Features:**
- âœ… AI-powered classification (Product/Service/Both) with confidence scoring
- âœ… LLM-Based Deadline Extraction with timezone support
- âœ… SAM.gov Page Integration: Raw page text included in analysis
- âœ… Optional file uploads with SAM.gov URL analysis
- âœ… Configurable Analysis: Enable/disable via UI (disabled by default)

</details>

<details>
<summary><strong>ğŸ’» Modern Frontend</strong></summary>

- âœ… React 18 with Vite
- âœ… TailwindCSS for professional styling
- âœ… Responsive, mobile-friendly design
- âœ… Real-time status updates with polling
- âœ… Intuitive UI with icon-based navigation

</details>

<details>
<summary><strong>âš¡ Background Processing</strong></summary>

- âœ… Celery task queue for async operations
- âœ… Redis message broker
- âœ… Real-time progress tracking
- âœ… Automatic retry on failures

</details>

<details>
<summary><strong>ğŸ’¾ Data Management</strong></summary>

- âœ… Organized file storage (documents/uploads)
- âœ… Secure document viewing/downloading
- âœ… Complete data cleanup on opportunity deletion
- âœ… Debug extraction files for troubleshooting

</details>

### Phase 2 - Complete âœ…

<details>
<summary><strong>ğŸ­ Manufacturer & Dealer Research Automation</strong></summary>

**Phase 1: Document-Based Extraction:**
- âœ… LLM extracts manufacturers and dealers from documents (CLINs, BOMs, Qualified Sources)
- âœ… Combined extraction with CLINs and deadlines in single LLM call for efficiency
- âœ… Extracts: manufacturer name, CAGE code, part numbers, NSNs, dealer company names
- âœ… Automatically triggered during document analysis

**Phase 2: External Web Research:**
- âœ… LLM-guided web search for manufacturer websites and authorized dealers
- âœ… Uses Playwright for web scraping and navigation
- âœ… Finds manufacturer official websites and sales contact emails
- âœ… Identifies top 8 authorized dealers/distributors with:
  - Company name, website URL, sales contact email
  - Current retail pricing (if publicly available)
  - Stock status and availability
  - Rank score (1-10) based on legitimacy
- âœ… SAM.gov verification (placeholder - needs API integration)
- âœ… Automatically triggered after Phase 1 completes
- âœ… Uses reference guide for legitimate search strategies

**Data Storage:**
- âœ… Manufacturers stored with research status, source, verification status
- âœ… Dealers stored with pricing, stock status, rank scores, authorization status
- âœ… Full relationship mapping (manufacturers â†’ dealers â†’ CLINs â†’ opportunities)
- âœ… Frontend displays all research results with proper formatting

</details>

### Phase 2 - Planned (Future)

- ğŸ“§ Email integration (Gmail, Outlook) with automated quote inquiries
- ğŸ“… Calendar integration (Google Calendar, iCal, Outlook) with automatic deadline events
- ğŸ“ Quote generation and review system

### Phase 3 - Planned

- ğŸ“‹ PDF form automation (SF1449 autofill)
- ğŸ“Š Advanced reporting dashboard
- ğŸ”„ Multi-opportunity batch processing

---

## ğŸ› ï¸ Tech Stack

<details>
<summary><strong>âš™ï¸ Backend Technologies</strong></summary>

| Technology | Purpose |
|------------|---------|
| ğŸ **FastAPI** | Modern, fast web framework |
| ğŸ—„ï¸ **PostgreSQL** | Relational database |
| ğŸ”— **SQLAlchemy** | ORM for database operations |
| ğŸ”„ **Alembic** | Database migrations |
| âš¡ **Celery** | Distributed task queue |
| ğŸ”´ **Redis** | Message broker and cache |
| ğŸ­ **Playwright** | Web scraping and automation |

</details>

<details>
<summary><strong>ğŸ“„ Document Processing</strong></summary>

| Library | Purpose |
|---------|---------|
| ğŸ“‘ **pdfplumber** | PDF text extraction (text-based PDFs) |
| ğŸ“„ **PyPDF2** | PDF manipulation and splitting |
| â˜ï¸ **Google Document AI** | Cloud-based OCR for scanned PDFs |
| ğŸ‘ï¸ **pytesseract** | Local OCR with image preprocessing |
| ğŸ–¼ï¸ **opencv-python** | Image preprocessing for OCR |
| ğŸ–¨ï¸ **pdf2image** | PDF to image conversion for OCR |
| ğŸ“ **python-docx** | Word document parsing |
| ğŸ“Š **python-pptx** | PowerPoint document parsing |
| ğŸ“ˆ **openpyxl** | Excel file handling |
| ğŸ“‹ **pandas** | CSV and advanced Excel processing |

</details>

<details>
<summary><strong>ğŸ¤– AI & Machine Learning</strong></summary>

| Technology | Purpose |
|------------|---------|
| ğŸ§  **Claude 3 Haiku (Anthropic)** | Primary LLM for CLIN extraction, deadline extraction, and classification |
| ğŸš€ **Groq + LangChain** | Fallback LLM-powered extraction |
| ğŸ”— **LangChain** | LLM orchestration framework with structured output support |
| ğŸ“š **spaCy** | NLP for classification |
| ğŸ¯ **scikit-learn** | Machine learning algorithms |
| ğŸ”„ **Transformers** | Pre-trained NLP models |

</details>

<details>
<summary><strong>ğŸ’» Frontend Technologies</strong></summary>

| Technology | Purpose |
|------------|---------|
| âš›ï¸ **React 18** | UI library |
| âš¡ **Vite** | Build tool and dev server |
| ğŸ¨ **TailwindCSS** | Utility-first CSS framework |
| ğŸ§­ **React Router** | Client-side routing |
| ğŸ“¡ **Axios** | HTTP client for API calls |

</details>

---

## ğŸ“¥ Installation

### Prerequisites

```bash
# Required Software
- Python 3.12+
- Node.js 18.x+
- PostgreSQL 14+
- Redis 6+
- Git
```

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd sam-project
```

### Step 2: Backend Setup

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium
```

### Step 3: Database Setup

```bash
# Automated setup (recommended)
./scripts/setup_database.sh

# Or manual setup:
# 1. Install PostgreSQL
# 2. Create database and user
# 3. Update .env with DATABASE_URL
```

### Step 4: Environment Configuration

Create `.env` file with required variables:

```env
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/samgov_db

# Redis
REDIS_URL=redis://localhost:6379/0

# JWT Authentication
JWT_SECRET_KEY=your-secret-key-here
SECRET_KEY=your-app-secret-key

# CLIN Extraction Settings
# Anthropic Claude (Primary LLM for CLIN extraction)
ANTHROPIC_API_KEY=your-anthropic-api-key
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Groq (Fallback LLM for CLIN extraction)
GROQ_API_KEY=your-groq-api-key
GROQ_MODEL=llama-3.1-70b-versatile

# Text Extraction Settings
# Google Document AI (for high-quality OCR and text extraction from scanned PDFs)
GOOGLE_SERVICE_ACCOUNT_JSON=extras/your-service-account.json
GOOGLE_PROJECT_ID=your-project-id
GOOGLE_PROCESSOR_ID=your-processor-id
GOOGLE_LOCATION=us
GOOGLE_DOCAI_ENABLED=True
```

### Step 5: Run Migrations

```bash
# Automated (recommended)
./scripts/run_migrations.sh

# Or manually
alembic upgrade head
```

### Step 6: Frontend Setup

```bash
cd frontend
npm install
cd ..
```

---

## ğŸš€ Quick Start

### Automated Start (Recommended)

```bash
# Start all services (backend, frontend, Celery worker)
./start.sh
```

The script automatically:
- âœ… Checks prerequisites
- âœ… Verifies Python packages are installed
- âœ… Ensures data directories exist
- âœ… Verifies database connection
- âœ… Runs migrations (with fallback to direct alembic)
- âœ… Starts all required services (backend, frontend, Celery worker)

### Manual Start

```bash
# Terminal 1: Backend
source venv/bin/activate
uvicorn backend.app.main:app --host 0.0.0.0 --port 8000 --reload

# Terminal 2: Celery Worker
source venv/bin/activate
celery -A backend.app.core.celery_app worker --loglevel=info

# Terminal 3: Frontend
cd frontend
npm run dev
```

### Access Application

| Service | URL |
|---------|-----|
| ğŸŒ **Frontend** | http://localhost:5173 |
| ğŸ”Œ **Backend API** | http://localhost:8000 |
| ğŸ“– **API Docs (Swagger)** | http://localhost:8000/docs |
| ğŸ“š **API Docs (ReDoc)** | http://localhost:8000/redoc |
| â¤ï¸ **Health Check** | http://localhost:8000/health |

### Stop Services

```bash
./stop.sh
```

---

## ğŸ“ Project Structure

```
sam-project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/              # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py       # Authentication
â”‚   â”‚   â”‚   â”œâ”€â”€ opportunities.py
â”‚   â”‚   â”‚   â””â”€â”€ router.py
â”‚   â”‚   â”œâ”€â”€ core/             # Configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â”‚   â””â”€â”€ celery_app.py
â”‚   â”‚   â”œâ”€â”€ models/           # Database models
â”‚   â”‚   â”‚   â”œâ”€â”€ opportunity.py
â”‚   â”‚   â”‚   â”œâ”€â”€ clin.py
â”‚   â”‚   â”‚   â”œâ”€â”€ deadline.py
â”‚   â”‚   â”‚   â”œâ”€â”€ manufacturer.py
â”‚   â”‚   â”‚   â””â”€â”€ dealer.py
â”‚   â”‚   â”œâ”€â”€ schemas/          # Pydantic schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ opportunity.py
â”‚   â”‚   â”‚   â”œâ”€â”€ clin.py
â”‚   â”‚   â”‚   â”œâ”€â”€ deadline.py
â”‚   â”‚   â”‚   â”œâ”€â”€ manufacturer.py
â”‚   â”‚   â”‚   â””â”€â”€ dealer.py
â”‚   â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sam_gov_scraper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ document_downloader.py  # Smart download with disclaimer handling
â”‚   â”‚   â”‚   â”œâ”€â”€ document_analyzer.py    # Facade for text extraction and CLIN detection
â”‚   â”‚   â”‚   â”œâ”€â”€ text_extractor.py      # Text extraction from all formats
â”‚   â”‚   â”‚   â”œâ”€â”€ clin_extractor.py      # CLIN detection (Claude + Groq) - also extracts manufacturers/dealers
â”‚   â”‚   â”‚   â”œâ”€â”€ research_service.py    # Database persistence for manufacturers and dealers
â”‚   â”‚   â”‚   â””â”€â”€ llm_external_research_service.py  # Phase 2: LLM-guided external web research
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â””â”€â”€ migrations/           # Alembic migrations
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/            # Page components
â”‚   â”‚   â”œâ”€â”€ components/       # Reusable components
â”‚   â”‚   â”œâ”€â”€ contexts/         # React contexts
â”‚   â”‚   â””â”€â”€ utils/            # Utilities
â”‚   â””â”€â”€ public/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/            # Downloaded documents
â”‚   â”œâ”€â”€ uploads/              # User uploads
â”‚   â””â”€â”€ debug_extracts/       # Debug extraction files
â”œâ”€â”€ scripts/                  # Setup scripts
â”œâ”€â”€ logs/                     # Application logs
â”œâ”€â”€ start.sh                  # Start script
â””â”€â”€ stop.sh                   # Stop script
```

---

## ğŸ“– API Documentation

### Interactive Documentation

Once the backend is running:

- **ğŸ“– Swagger UI**: http://localhost:8000/docs
  - Interactive API explorer
  - Try endpoints directly
  - Full request/response schemas

- **ğŸ“š ReDoc**: http://localhost:8000/redoc
  - Clean, readable format
  - Searchable documentation

### Core Endpoints

#### Authentication

```
POST   /api/v1/auth/register    # User registration
POST   /api/v1/auth/login       # Login (returns JWT)
GET    /api/v1/auth/me          # Current user (protected)
POST   /api/v1/auth/logout      # Logout (protected)
```

#### Opportunities

```
GET    /api/v1/opportunities                    # List opportunities
POST   /api/v1/opportunities                    # Create opportunity (with optional file uploads)
GET    /api/v1/opportunities/{id}               # Get details with CLINs, manufacturers, dealers
DELETE /api/v1/opportunities/{id}               # Delete opportunity and files
GET    /api/v1/opportunities/{id}/documents/{doc_id}/view  # View document
```

### Example Usage

<details>
<summary><strong>ğŸ“ Create Opportunity with File Upload</strong></summary>

```bash
# 1. Login
curl -X POST http://localhost:8000/api/v1/auth/login \
  -H "Content-Type: application/json" \
  -d '{"email": "user@example.com", "password": "password"}'

# Response: {"access_token": "eyJ...", "token_type": "bearer"}

# 2. Create Opportunity with Files and Analysis Options
curl -X POST http://localhost:8000/api/v1/opportunities \
  -H "Authorization: Bearer eyJ..." \
  -F "sam_gov_url=https://sam.gov/workspace/contract/opp/.../view" \
  -F "enable_document_analysis=true" \
  -F "enable_clin_extraction=true" \
  -F "files=@/path/to/document1.pdf" \
  -F "files=@/path/to/document2.docx"
```

</details>

---

## ğŸ’» Development

### Database Migrations

```bash
# Create migration
alembic revision --autogenerate -m "Description"

# Apply migrations
alembic upgrade head

# Rollback
alembic downgrade -1
```

### Logs

| Service | Log File |
|---------|----------|
| ğŸ”§ Backend | `logs/backend.log` |
| âš¡ Celery | `logs/celery.log` |
| ğŸ’» Frontend | `logs/frontend.log` |

### Test Scripts

```bash
# Test Phase 1: Document extraction (CLINs, deadlines, manufacturers, dealers)
python scripts/test_opportunity_205.py <opportunity_id>

# Test Phase 2: External research (requires Phase 1 to be complete)
python scripts/test_phase2_external_research.py <opportunity_id>

# List available opportunities
python scripts/test_opportunity_205.py --list
```

### Debug Extracts

Extracted text and analysis results are saved to:

```
data/debug_extracts/opportunity_{id}/
  â”œâ”€â”€ {doc_id}_{filename}_extracted.txt
  â”œâ”€â”€ {doc_id}_{filename}_clins.txt
  â””â”€â”€ analysis_summary.txt
```

---

## ğŸš¢ Deployment

### Digital Ocean App Platform

1. Push code to Git repository
2. Connect repository to Digital Ocean App Platform
3. Configure environment variables
4. Deploy

### Environment Variables for Production

Ensure all required environment variables are set:
- Database connection string
- Redis URL
- JWT secrets
- API keys (Anthropic, Groq, Google Document AI)
- Playwright browser installation

### Docker

```bash
# Build and run
docker-compose up -d

# Or build individually
docker build -f Dockerfile.backend -t samgov-backend .
docker build -f Dockerfile.frontend -t samgov-frontend .
```

---

## ğŸ“Š Project Status

### Phase 1: Foundation & Core Scraping âœ… **COMPLETE**

| Feature | Status |
|---------|--------|
| ğŸ” User Authentication | âœ… |
| ğŸŒ SAM.gov Scraping | âœ… |
| ğŸ“¥ Smart Document Downloads | âœ… |
| ğŸ‘¥ Contact Info Extraction | âœ… |
| ğŸ“„ Multi-format Text Extraction | âœ… |
| â˜ï¸ Google Document AI Integration | âœ… |
| ğŸ‘ï¸ OCR Support | âœ… |
| ğŸ“Š Document Analysis | âœ… |
| ğŸ“‹ CLIN Extraction | âœ… |
| â° Deadline Extraction | âœ… |
| ğŸ“¤ File Upload | âœ… |
| ğŸ’» Frontend UI | âœ… |

**Phase 1 is 100% complete** - All MVP requirements met!

### Phase 2: Research & Automation âœ… **COMPLETE**

| Feature | Status |
|---------|--------|
| ğŸ“„ Document-based manufacturer/dealer extraction | âœ… |
| ğŸŒ LLM-guided external web research | âœ… |
| ğŸ­ Manufacturer website discovery | âœ… |
| ğŸ“§ Sales contact extraction | âœ… |
| ğŸª Authorized dealer identification | âœ… |
| ğŸ’° Pricing extraction | âœ… |
| ğŸ“¦ Stock status tracking | âœ… |
| â­ Legitimacy ranking | âœ… |
| ğŸ’¾ Database persistence | âœ… |
| ğŸ–¥ï¸ Frontend display | âœ… |

**Phase 2 Research Automation is 100% complete** - All manufacturer and dealer research features implemented!

### Recent Enhancements

- âœ… **Unified CLIN & Deadline Extraction**: All documents + SAM.gov page text combined in single LLM request
- âœ… **Delivery Requirements Integration**: Delivery addresses, special instructions, and timelines extracted
- âœ… **Two-Pass Extraction**: Automatic second pass fills missing fields when 20%+ are null
- âœ… **Robust JSON Parsing**: Advanced error recovery for malformed/truncated LLM responses
- âœ… **SAM.gov Page Integration**: Raw page text included in analysis
- âœ… **Configurable Analysis**: Document analysis and CLIN extraction can be enabled/disabled via UI
- âœ… **Smart Text Extraction**: Google Document AI for scanned PDFs with intelligent routing
- âœ… **Improved OCR**: pytesseract with advanced image preprocessing
- âœ… **LLM Fallback**: Claude 3 Haiku as primary, Groq Llama 3.1 as fallback
- âœ… **Disclaimer Handling**: Automatic detection and handling of disclaimer/agreement pages
- âœ… **Recursive Navigation**: Smart depth tracking (max 4 levels) for multi-page document downloads
- âœ… **Phase 2 Research Automation**: Two-phase manufacturer/dealer research
- âœ… **LLM-Guided External Research**: Intelligent web search using LLM
- âœ… **Dealer Discovery**: Automated identification of top 8 authorized dealers
- âœ… **Database Schema Consistency**: Complete manufacturer and dealer schemas
- âœ… **Frontend Display**: Full UI for displaying manufacturers and dealers

---

## ğŸ”„ How It Works

<details>
<summary><strong>ğŸ“Š Analysis Pipeline</strong></summary>

1. **ğŸ“¥ Input**: User provides SAM.gov URL (+ optional files) with analysis options

2. **ğŸŒ Scraping**: Playwright extracts metadata and downloads attachments
   - Handles disclaimer/agreement pages automatically
   - Recursive navigation with depth tracking (max 4 levels)
   - Smart PDF detection and download

3. **ğŸ“„ Document Analysis** (if enabled):
   - **Text Extraction**: 
     - Text-based PDFs â†’ pdfplumber
     - Scanned PDFs â†’ Google Document AI (with pytesseract fallback)
     - Other formats â†’ Format-specific extractors
   - **Document Classification**: Documents routed by type (SF1449, SF30, SOW, etc.)

4. **ğŸ“‹ CLIN, Deadline, Manufacturer & Dealer Extraction** (if enabled):
   - **Combined Processing**: All document texts + SAM.gov page text sent to LLM in single request
   - **Primary**: Claude 3 Haiku extracts CLINs, deadlines, manufacturers, and dealers together
   - **Fallback**: Groq Llama 3.1 if Claude fails
   - **Two-Pass System**: If 20%+ fields are missing, second pass fills missing values
   - **Robust Parsing**: Advanced JSON error recovery handles malformed/truncated responses
   - Extracts: CLIN numbers, product details, quantities, manufacturers (name, CAGE code, part numbers), dealers (company names), scope of work, service requirements, delivery addresses, special delivery instructions, delivery timelines, deadlines

5. **ğŸ¤– Classification**: AI determines Product/Service/Both with confidence scoring

6. **ğŸ’¾ Storage**: All data saved to database with proper deduplication (CLINs, deadlines, manufacturers, dealers)

7. **ğŸŒ Phase 2 External Research** (automatic after Phase 1):
   - LLM determines best search strategy for each manufacturer
   - Playwright performs web searches and navigates to manufacturer websites
   - Extracts website URLs, contact emails, phone numbers
   - Finds authorized dealers with pricing and stock status
   - Ranks dealers by legitimacy (1-10 score)

8. **ğŸ–¥ï¸ Display**: Data displayed in UI with collapsible CLIN sections, manufacturer/dealer sections, and professional formatting

</details>

<details>
<summary><strong>ğŸ“‹ CLIN & Deadline Extraction Process</strong></summary>

**Unified Extraction Approach:**
- All documents + SAM.gov page text are combined and sent to LLM in a single request
- Claude 3 Haiku (primary) extracts CLINs, deadlines, and delivery requirements together
- Groq Llama 3.1 (fallback) used if Claude fails
- LangChain with structured output ensures JSON format compliance

**Extracted Fields:**
- **CLIN Data**: Numbers, descriptions, quantities, units, product names, manufacturers, part/model numbers, drawing numbers, contract types
- **Scope & Requirements**: Complete scope of work text, detailed service requirements
- **Delivery Information**: Complete delivery addresses, special delivery instructions, delivery timelines
- **Deadlines**: Submission deadlines, question deadlines, with dates, times, timezones, and types
- **Manufacturers**: Name, CAGE code, part numbers, NSNs (from documents)
- **Dealers**: Company names (from documents)

**Two-Pass System:**
- First pass extracts all available data
- If 20%+ of important fields are null, second pass specifically targets missing fields
- Ensures maximum data completeness

**Error Handling:**
- Robust JSON parsing with multiple fallback strategies
- Handles truncated/malformed JSON responses
- Extracts partial data even from incomplete responses
- Individual CLIN object extraction if outer JSON structure is broken

</details>

<details>
<summary><strong>ğŸ­ Manufacturer & Dealer Research Process</strong></summary>

**Phase 1: Document-Based Extraction:**
- Manufacturers and dealers extracted from documents alongside CLINs and deadlines
- Single LLM call processes all documents + SAM.gov page text
- Extracts manufacturer names, CAGE codes, part numbers, NSNs
- Extracts dealer company names from Qualified Sources, BOMs, and CLINs
- Data saved with `research_source="document_extraction"`

**Phase 2: External Web Research (Automatic):**
- Triggered automatically after Phase 1 completes
- For each manufacturer found in documents:
  1. LLM analyzes manufacturer info and determines best search strategy
  2. Uses reference guide for legitimate search methods
  3. Playwright performs Google searches and navigates to websites
  4. Extracts manufacturer website, contact email, phone, address
  5. Searches for authorized dealers/distributors
  6. Extracts dealer info: company name, website, contact email, pricing, stock status
  7. Ranks dealers by legitimacy (1-10 score)
  8. Verifies websites and checks SAM.gov status (placeholder)
- Results saved with `research_source="external_search"` or `"document_extraction,external_search"`

**Data Flow:**
- Database Models â†’ Data Saving â†’ API Schemas â†’ API Endpoint â†’ Frontend Display
- All layers consistent: manufacturers and dealers properly serialized and displayed

</details>

---

## ğŸ“„ License

Proprietary - All rights reserved

---

<div align="center">

**ğŸ›ï¸ Built for Government Contractors**

[â¬†ï¸ Back to Top](#-samgov-ai)

</div>
